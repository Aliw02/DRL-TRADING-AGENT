# config/config_sac.yaml
# FINAL, HIGH-THROUGHPUT CONFIGURATION

training:
  learning_rate: 0.0002
  buffer_size: 75000
  batch_size: 128
  tau: 0.005
  gamma: 0.99
  
  # Continuous (Online) Training Strategy
  train_freq: [1, "step"]
  gradient_steps: 1
  learning_starts: 1000
  
  walk_forward:
    n_splits: 5
    min_train_size: 40000
    test_size: 30000
    timesteps_per_split: 75000
    eval_freq: 5000

  fine_tuning:
    recent_data_years: 2.5
    total_timesteps: 100000
    learning_rate: 0.00002

environment:
  initial_balance: 10000
  # --- STRATEGIC ADJUSTMENT: Reduce sequence length ---
  sequence_length: 30 # A shorter sequence is much faster to process
  commission_pct: 0.0005

model:
  features_dim: 256
  # CNN channels can be slightly increased as the model is simpler
  cnn_out_channels: 64

specialist_training:
  min_samples_per_regime: 5000