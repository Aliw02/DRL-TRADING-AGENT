# config/config_sac.yaml
# FINAL AND GUARANTEED "BATCH TRAINING" CONFIGURATION

training:
  learning_rate: 0.0003
  buffer_size: 75000
  batch_size: 128
  tau: 0.005
  gamma: 0.99
  
  # --- THE GUARANTEED FIX: CHANGE THE TRAINING STRATEGY ---
  # Instead of training after every step, we collect a large chunk of data first,
  # then perform all the training updates at once. This is vastly more efficient.
  
  # Collect 1024 steps from the environment...
  train_freq: [1024, "step"] # <<-- THE MOST CRITICAL CHANGE IN THE ENTIRE PROJECT
  
  # ...then perform 1024 gradient updates.
  gradient_steps: -1 # <<-- Tells the model to match gradient_steps to train_freq (i.e., 1024)
  
  learning_starts: 10000 
  
  walk_forward:
    n_splits: 5
    min_train_size: 40000
    test_size: 30000
    timesteps_per_split: 150000

  fine_tuning:
    recent_data_years: 2.5
    total_timesteps: 200000
    learning_rate: 0.00001

environment:
  initial_balance: 10000
  sequence_length: 60
  commission_pct: 0.0005
  turnover_penalty: 0.001

model:
  features_dim: 256
  cnn_out_channels: 64
  transformer_n_heads: 4
  transformer_n_layers: 2
  transformer_dropout: 0.1